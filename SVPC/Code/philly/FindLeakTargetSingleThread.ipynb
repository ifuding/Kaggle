{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from scipy.stats import mode, skew, kurtosis, entropy\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# import dask.dataframe as dd\n",
    "# from dask.multiprocessing import get\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "import concurrent.futures\n",
    "\n",
    "import pickle\n",
    "\n",
    "from leak_cols import *\n",
    "leak_list = LEAK_LIST\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "path = \"../../Data/\"\n",
    "train = pd.read_csv(path + \"train.csv\", index_col = 'ID')\n",
    "test = pd.read_csv(path + \"test.csv\", index_col = 'ID')\n",
    "\n",
    "debug = False\n",
    "if debug:\n",
    "    train = train[:1000]\n",
    "    test = test[:1000]\n",
    "\n",
    "transact_cols = [f for f in train.columns if f not in [\"ID\", \"target\"]]\n",
    "y = np.log1p(train[\"target\"]).values\n",
    "\n",
    "cols = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1',\n",
    "       '15ace8c9f', 'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9',\n",
    "       'd6bb78916', 'b43a7cfd5', '58232a6fb', '1702b5bf0', '324921c7b', \n",
    "       '62e59a501', '2ec5b290f', '241f0f867', 'fb49e4212',  '66ace2992',\n",
    "       'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', '1931ccfdd', \n",
    "       '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a',\n",
    "       '6619d81fc', '1db387535', 'fc99f9426', '91f701ba2',  '0572565c2',\n",
    "       '190db8488',  'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98'] \n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IsTrain = False\n",
    "CPU_CORES = 1\n",
    "NZ_NUM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _get_leak(df, cols, search_ind, lag=0):\n",
    "#     \"\"\" To get leak value, we do following:\n",
    "#        1. Get string of all values after removing first two time steps\n",
    "#        2. For all rows we shift the row by two steps and again make a string\n",
    "#        3. Just find rows where string from 2 matches string from 1\n",
    "#        4. Get 1st time step of row in 3 (Currently, there is additional condition to only fetch value if we got exactly one match in step 3)\"\"\"\n",
    "#     f1 = [] #cols[:((lag+2) * -1)]\n",
    "#     f2 = [] #cols[(lag+2):]\n",
    "#     for ef in leak_list:\n",
    "#         f1 += ef[:((lag+2) * -1)]\n",
    "#         f2 += ef[(lag+2):]\n",
    "#     series_str = df[f2]\n",
    "#     nz = series_str.apply(lambda x: len(x[x!=0]), axis=1)\n",
    "#     series_str = series_str[nz >= NZ_NUM]\n",
    "#     series_str = series_str.apply(lambda x: \"_\".join(x.round(2).astype(str)), axis=1)\n",
    "#     series_str = series_str.drop_duplicates(keep = False) #[(~series_str.duplicated(keep = False)) | (df[cols[lag]] != 0)]\n",
    "#     series_shifted_str = df.loc[search_ind, f1].apply(lambda x: \"_\".join(x.round(2).astype(str)), axis=1)\n",
    "#     target_rows = series_shifted_str.progress_apply(lambda x: np.where(x == series_str.values)[0])\n",
    "#     # print(target_rows)\n",
    "#     # del series_str, series_shifted_str\n",
    "#     # target_vals = target_rows.apply(lambda x: df.loc[series_str.index[x[0]], cols[lag]] if len(x)==1 else 0)\n",
    "#     target_vals = target_rows.apply(lambda x: df.loc[series_str.index[x[0]], cols[lag]] if len(x) == 1 else 0)\n",
    "#         # if (len(x) > 0 and df.loc[series_str.index[x], cols[lag]].nunique() == 1) else 0)\n",
    "#     return target_vals, lag\n",
    "\n",
    "def _get_leak(df, cols, search_ind, lag=0):\n",
    "    \"\"\" To get leak value, we do following:\n",
    "       1. Get string of all values after removing first two time steps\n",
    "       2. For all rows we shift the row by two steps and again make a string\n",
    "       3. Just find rows where string from 2 matches string from 1\n",
    "       4. Get 1st time step of row in 3 (Currently, there is additional condition to only fetch value if we got exactly one match in step 3)\"\"\"\n",
    "    f1 = [] #cols[:((lag+2) * -1)]\n",
    "    f2 = [] #cols[(lag+2):]\n",
    "    for ef in leak_list:\n",
    "        f1 += ef[:((lag+2) * -1)]\n",
    "        f2 += ef[(lag+2):]\n",
    "    series_str = df[f2]\n",
    "    nz = series_str.apply(lambda x: len(x[x!=0]), axis=1)\n",
    "    series_str = series_str[nz >= NZ_NUM]\n",
    "    series_str['key'] = series_str.apply(lambda x: \"_\".join(x.round(2).astype(str)), axis=1)\n",
    "    series_str = series_str[~series_str.duplicated(['key'], keep=False)] \n",
    "    series_str['pred'] = df[cols[lag]]\n",
    "    series_str = series_str[['key', 'pred']]\n",
    "    \n",
    "    series_shifted_str = df.loc[search_ind, f1]\n",
    "    series_shifted_str['key'] = series_shifted_str.apply(lambda x: \"_\".join(x.round(2).astype(str)), axis=1)\n",
    "    series_shifted_str = series_shifted_str.reset_index()[['key', 'ID']]\n",
    "    \n",
    "    target_vals = series_shifted_str.merge(series_str, how='left', on='key').set_index('ID')\n",
    "#     print (target_vals.head())\n",
    "    target_vals = target_vals.pred.fillna(0)\n",
    "    print ('Done')\n",
    "    return target_vals, lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used_col length:  3001\n",
      "begin_ind:  0 end_ind:  1 search_ind_len:  49342\n",
      "Done\n",
      "Find leak in train and test:  0 2943 leak train right:  0 0\n",
      "begin_ind:  1 end_ind:  2 search_ind_len:  46399\n",
      "Done\n",
      "Find leak in train and test:  0 4212 leak train right:  0 0\n",
      "begin_ind:  2 end_ind:  3 search_ind_len:  45130\n"
     ]
    }
   ],
   "source": [
    "def get_all_leak(df, cols=None, nlags=15):\n",
    "    \"\"\"\n",
    "    We just recursively fetch target value for different lags\n",
    "    \"\"\"\n",
    "    df =  df.copy()\n",
    "#     print(df.head())\n",
    "    if True: #with Pool(processes=CPU_CORES) as p:\n",
    "        begin_ind = 0\n",
    "        end_ind = 0\n",
    "        leak_target = pd.Series(0, index = df.index)\n",
    "        while begin_ind < nlags:\n",
    "            end_ind = min(begin_ind + CPU_CORES, nlags)\n",
    "            search_ind = (leak_target == 0)\n",
    "            # print(search_ind)\n",
    "            print('begin_ind: ', begin_ind, 'end_ind: ', end_ind, \"search_ind_len: \", search_ind.sum())\n",
    "#             res = [p.apply_async(_get_leak, args=(df, cols, search_ind, i)) for i in range(begin_ind, end_ind)]\n",
    "#             for r in res:\n",
    "#                 target_vals, lag = r.get()\n",
    "# #                 print ('target_vale', target_vals.head())\n",
    "#                 # leak_target[target_vals.index] = target_vals\n",
    "#                 df['leak_target_' + str(lag)] = target_vals\n",
    "            target_vals, lag = _get_leak(df, cols, search_ind, begin_ind)\n",
    "            df['leak_target_' + str(lag)] = target_vals\n",
    "            for i in range(begin_ind, end_ind):\n",
    "                leak_target[leak_target == 0] = df.loc[leak_target == 0, 'leak_target_' + str(i)]\n",
    "            leak_train = 0 #leak_target[train.index]\n",
    "            leak_train_len = 0 #leak_train[leak_train != 0].shape[0]\n",
    "            leak_test_len = 0 #leak_target[test.index][leak_target != 0].shape[0]\n",
    "            leak_train_right_len = 0 #leak_train[leak_train.round(0) == train['target'].round(0)].shape[0]\n",
    "            leak_train_right_ratio = 0 #leak_train_right_len / leak_train_len\n",
    "            if IsTrain:\n",
    "                leak_train = leak_target[train.index]\n",
    "#                 print (leak_train.head())\n",
    "                leak_train_len = leak_train[leak_train != 0].shape[0]\n",
    "                leak_train_right_len = leak_train[leak_train.round(0) == train['target'].round(0)].shape[0]\n",
    "                leak_train_right_ratio = leak_train_right_len / leak_train_len\n",
    "            else:\n",
    "                leak_test_len = leak_target[test.index][leak_target != 0].shape[0]\n",
    "            print('Find leak in train and test: ', leak_train_len, leak_test_len, \\\n",
    "                \"leak train right: \", leak_train_right_len, leak_train_right_ratio)\n",
    "            begin_ind = end_ind\n",
    "    # for i in range(nlags):\n",
    "    #     df.loc[df['leak_target'] == 0, 'leak_target'] = df.loc[df['leak_target'] == 0, 'leak_target_' + str(i)]\n",
    "    df['leak_target'] = leak_target\n",
    "    return df\n",
    "\n",
    "def get_pred(data, lag=2):\n",
    "    d1 = data[FEATURES[:-lag]].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n",
    "    d2 = data[FEATURES[lag:]].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n",
    "    d2['pred'] = data[FEATURES[lag - 2]]\n",
    "    d3 = d2[~d2.duplicated(['key'], keep=False)]\n",
    "    return d1.merge(d3, how='left', on='key').pred.fillna(0)\n",
    "\n",
    "def get_all_pred(data, max_lag):\n",
    "    target = pd.Series(index=data.index, data=np.zeros(data.shape[0]))\n",
    "    for lag in range(2, max_lag + 1):\n",
    "        pred = get_pred(data, lag)\n",
    "        mask = (target == 0) & (pred != 0)\n",
    "        target[mask] = pred[mask]\n",
    "    return target\n",
    "\n",
    "test[\"target\"] = 0 #train[\"target\"].mean()\n",
    "\n",
    "# all_df = pd.concat([train[[\"ID\", \"target\"] + cols], test[[\"ID\", \"target\"]+ cols]]).reset_index(drop=True)\n",
    "# all_df = pd.concat([train[[\"target\"] + cols], test[[\"target\"]+ cols]]) #.reset_index(drop=True)\n",
    "# all_df.head()\n",
    "\n",
    "NLAGS = 38 #Increasing this might help push score a bit\n",
    "used_col = [\"target\"] + [col for cols in leak_list for col in cols]\n",
    "print ('used_col length: ', len(used_col))\n",
    "if IsTrain:\n",
    "    all_df = get_all_leak(train[used_col], cols=cols, nlags=NLAGS)\n",
    "else:\n",
    "    all_df = get_all_leak(test[used_col], cols=cols, nlags=NLAGS)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time_label = time.strftime('_%Y_%m_%d_%H_%M_%S', time.gmtime())\n",
    "    \n",
    "if IsTrain:\n",
    "    all_df[['target', 'leak_target']].to_csv(path + 'train_target_leaktarget_' + str(NLAGS) + \"_\" + str(NZ_NUM) + time_label +  '.csv')\n",
    "else:\n",
    "    all_df[['target', 'leak_target']].to_csv(path + 'test_target_leaktarget_' + str(NLAGS) + \"_\" + str(NZ_NUM) + time_label + '.csv')\n",
    "# with open(path + 'leak_target_' + str(NLAGS) + '.pickle', 'wb+') as handle:\n",
    "#     pickle.dump(all_df[['target', 'leak_target']], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "sub = pd.read_csv(path + 'sub_2018_08_13_03_19_33.csv', index_col = 'ID')\n",
    "leak_target = all_df['leak_target'][test.index]\n",
    "# print(leak_target)\n",
    "sub.loc[leak_target[leak_target != 0].index, 'target'] = leak_target[leak_target != 0]\n",
    "\n",
    "if not IsTrain:\n",
    "    sub.to_csv(path + 'leak_sub_' + str(NLAGS) + \"_\" + time_label + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
